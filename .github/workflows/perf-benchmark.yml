name: Performance benchmark

on:
  workflow_dispatch:
    inputs:
      docker-image:
        description: "Docker image used in tests"
        required: false
        type: string
        default: "ghcr.io/tenstorrent/tt-forge/tt-forge-slim:latest"
      project-filter:
        description: "Project filter"
        required: false
        type: choice
        options:
          - tt-forge-fe
          - tt-torch
          - tt-xla
          - All
        default: All
      test-filter:
        description: "Only run tests that contains"
        required: false
        type: string
      update-wheel:
        description: "Update wheel for the project"
        required: false
        type: boolean
      sh-runner:
        description: "Run on shared runner"
        required: false
        type: boolean
  workflow_call:
    inputs:
      docker-image:
        description: "Docker image used in tests"
        required: false
        default: "ghcr.io/tenstorrent/tt-forge/tt-forge-slim:latest"
        type: string
      project:
        description: "Project(s) to launch tests (space separated)"
        required: false
        type: string
      run_id_source:
        description: "Repo source for provided workflow run ID"
        required: false
        type: string
      run_id:
        description: "Workflow ID to use for the new version of wheel"
        required: false
        type: string
      ref:
        description: "Git ref to checkout"
        required: false
        type: string
      test-filter:
        description: "Only run tests that contains"
        required: false
        type: string
      sh-runner:
        description: "Run on shared runner"
        required: false
        type: boolean

jobs:
  filter-tests:
    runs-on: ubuntu-latest
    outputs:
      benchmark-matrix: ${{ steps.set-perf-benchmarks.outputs.perf-benchmarks }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        repository: 'tenstorrent/tt-forge'
        fetch-depth: 1
        ref: ${{ inputs.ref || github.ref }}
    - name: Filter Matrix
      id: set-perf-benchmarks
      shell: bash
      run: |
        if [ "${{ inputs.project-filter }}" == "All" ]; then
          project="tt-forge-fe tt-torch tt-xla"
        else
          project="${{ inputs.project || inputs.project-filter }}"
        fi

        filter='.[] | select(true'

        if [ -n "$project" ]; then
          pa=($project)
          f="false"
          for p in "${pa[@]}"; do
            f="$f or .project == \"$p\""
          done
          filter="$filter and ($f)"
        fi
        if [ -n "${{ inputs.test-filter }}" ]; then
          filter="$filter and (.name | contains(\"${{ inputs.test-filter }}\"))"
        fi

        matrix=$(jq -c "[${filter})]" .github/workflows/perf-bench-matrix.json)
        echo "Matrix: $matrix"
        if [ "$matrix" == "[]" ]; then
          echo "Error: No matching tests found in the matrix"
          exit 1
        fi
        echo "perf-benchmarks=$matrix" >> $GITHUB_OUTPUT

        # Set up the summary for the job
        echo "### Perf benchmarks inputs" >> $GITHUB_STEP_SUMMARY
        echo "Project(s): $project" >> $GITHUB_STEP_SUMMARY
        echo "Test filter: ${{ inputs.test-filter }}" >> $GITHUB_STEP_SUMMARY
        echo "Run ID source: ${{ inputs.run_id_source }}" >> $GITHUB_STEP_SUMMARY
        echo "Run ID: ${{ inputs.run_id }}" >> $GITHUB_STEP_SUMMARY
        echo "Shared runner: ${{ inputs.sh-runner }}" >> $GITHUB_STEP_SUMMARY

  run-perf-benchmarks:
    needs: filter-tests
    container:
      image: ${{ inputs.sh-runner && format('harbor.ci.tenstorrent.net/{0}', inputs.docker-image) || inputs.docker-image }}
      options: --device /dev/tenstorrent --user root
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /usr/local/bin:/usr/local/bin
        - /mnt/dockercache:/mnt/dockercache

    timeout-minutes: 60

    # tt-torch reqs are not needed by tt-torch, but rather by benchmarking infra, should be moved elsewhere: https://github.com/tenstorrent/tt-forge/issues/177
    strategy:
      fail-fast: false
      matrix:
        build: ${{ fromJson(needs.filter-tests.outputs.benchmark-matrix) }}

    runs-on: ${{ inputs.sh-runner && format('tt-ubuntu-2204-{0}-stable', matrix.build.runs-on) || fromJson(format('["{0}-perf", "in-service"]', matrix.build.runs-on)) }}

    env:
      # TODO: Revisit the addition of these env vars https://github.com/tenstorrent/tt-metal/issues/20161
      TRACY_NO_INVARIANT_CHECK: 1
      DOCKER_CACHE_ROOT: /mnt/dockercache

    name: "${{matrix.build.project }}-${{matrix.build.name }} benchmark (${{ inputs.sh-runner && format('{0}-shared', matrix.build.runs-on) || (matrix.build.runs-on) }}, ${{ matrix.build.bs }}, ${{ matrix.build.lp }})"
    steps:

    - uses: actions/checkout@v4
      with:
        repository: 'tenstorrent/tt-forge'
        fetch-depth: 1
        ref: ${{ inputs.ref || github.ref }}

    - name: Install system and python dependencies
      shell: bash
      run: |
        if [ "${{ matrix.build.project }}" == "tt-forge-fe" ]; then
          apt-get install -y -qq --no-install-recommends sqlite3
        fi
        if [ -n "${{ matrix.build.libreq }}" ]; then
          apt-get install -y -qq --no-install-recommends ${{ matrix.build.libreq }}
        fi

        source /home/forge/venv-${{ matrix.build.project }}/bin/activate
        if [ -n "${{ matrix.build.pyreq }}" ]; then
          pip install ${{ matrix.build.pyreq }}
        fi

    - name: Install upgraded wheel
      if: (inputs.project && inputs.run_id) || inputs.update-wheel
      uses: ./.github/actions/install-wheel
      with:
        project: ${{ matrix.build.project }}
        run_id: ${{ inputs.run_id }}
        run_id_source: ${{ inputs.run_id_source }}
        venv_bin_path: /home/forge/venv-${{ matrix.build.project }}/bin/activate

    - name: Fetch job id
      id: fetch-job-id
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "${{matrix.build.project }}-${{matrix.build.name }} benchmark (${{ inputs.sh-runner && format('{0}-shared', matrix.build.runs-on) || (matrix.build.runs-on) }}, ${{ matrix.build.bs }}, ${{ matrix.build.lp }})"

    - name: Set reusable strings
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}
      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "build-output-dir=$(pwd)/build" >> "$GITHUB_OUTPUT"
        echo "perf_report_path=$(pwd)/benchmark_reports" >> "$GITHUB_OUTPUT"

    # - name: Git safe dir
    #   run: git config --global --add safe.directory ${{ steps.strings.outputs.work-dir }}

    - name: Find workflow run ID for ttrt wheel download
      id: find-run
      shell: bash
      run: |
        source /home/forge/venv-${{ matrix.build.project }}/bin/activate
        project_name=$(echo "${{ matrix.build.project }}" | tr "-" "_" )
        pip freeze | grep "$project_name"
        version=$(pip freeze | grep -oP "$project_name.*?-\K[0-9]+\.[0-9]+\.[0-9]+\.dev[0-9]+")
        echo "version: $version"
        mlir_sha=""

        if [[ "${{ matrix.build.project }}" == "tt-torch" || "${{ matrix.build.project }}" == "tt-xla" ]]; then
          wget "https://raw.githubusercontent.com/tenstorrent/${{ matrix.build.project }}/${version}/third_party/CMakeLists.txt"
          echo "log after cmakelists download"
          ls -l
          mlir_sha=$(grep -E "set\(TT_MLIR_VERSION" CMakeLists.txt | grep -oP '"\K[^"]+' | head -n 1)
        elif [[ "${{ matrix.build.project }}" == "tt-forge-fe" ]]; then
          mlir_sha=$(curl -s "https://api.github.com/repos/tenstorrent/tt-forge-fe/contents/third_party/tt-mlir?ref=${version}" | \
            grep '"sha"' | head -1 | cut -d'"' -f4)
        fi

        echo "mlir_sha: $mlir_sha"

        RUN_ID=$(curl -s \
          -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
          "https://api.github.com/repos/tenstorrent/tt-mlir/actions/workflows/on-push.yml/runs?status=success&head_sha=${mlir_sha}&per_page=1" \
          | jq -r '.workflow_runs[0].id')
        echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
        echo "Workflow URL: https://github.com/tenstorrent/tt-mlir/actions/runs/$RUN_ID"
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    # ttrt version (sha) should be taken from release metadata or something
    - name: Download ttrt wheel
      uses: dawidd6/action-download-artifact@v11
      with:
        run_id: ${{ steps.find-run.outputs.run_id }}
        name: "(ttrt-whl|install-artifacts)-tracy"
        name_is_regexp: true
        repo: tenstorrent/tt-mlir
        check_artifacts: true
        path: ./

    - name: Install ttrt
      shell: bash
      run: |
        echo "Install ttrt"
        python -m venv ttrt-venv
        source ttrt-venv/bin/activate
        apt-get install -y -qq --no-install-recommends libtbb12 libcapstone4
        pip install ttrt-whl-tracy/ttrt*.whl --upgrade
        pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu

    - name: Untar ttmlir install directory
      shell: bash
      run: |
        mv install-artifacts-tracy/ install/
        cd install
        tar xvf artifact.tar
        cd ..

    - name: Run Perf Benchmark
      shell: bash
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        HF_HOME: /mnt/dockercache/huggingface
        HF_HUB_DISABLE_PROGRESS_BARS: 1
      run: |
        echo "Create perf report directory"
        mkdir -p ${{ steps.strings.outputs.perf_report_path }}
        source /home/forge/venv-${{ matrix.build.project }}/bin/activate

        if [ "${{ matrix.build.project }}" == "tt-torch" ]; then
          export TT_TORCH_SAVE_MLIR=TTIR
          export TT_TORCH_SAVE_BINARY=1
        fi

        echo "Run benchmark for ${{ matrix.build.project }} - ${{ matrix.build.name }}"
        python benchmark/benchmark.py -p ${{ matrix.build.project}} -m ${{ matrix.build.name }} -bs ${{ matrix.build.bs }} -df ${{ matrix.build.df }} -lp ${{ matrix.build.lp }} -ts ${{ matrix.build.ts }} -o ${{ steps.strings.outputs.perf_report_path }}/benchmark_${{ matrix.build.project }}_e2e_${{ matrix.build.name }}_${{ matrix.build.bs }}_${{ matrix.build.lp }}_${{ steps.fetch-job-id.outputs.job_id }}.json ${{ inputs.run_id_source && format('-r {0}', inputs.run_id_source) }}

        if [ "${{ matrix.build.project }}" != "tt-xla" ]; then
          echo "Dump ttir to report"
          if [ "${{ matrix.build.project }}" == "tt-torch" ]; then
            cp ./model_mlir/${{ matrix.build.name }}_ttir.mlir ${{ steps.strings.outputs.perf_report_path }}/ttir.mlir
          elif [ "${{ matrix.build.project }}" == "tt-forge-fe" ]; then
            cp ~/testify/ll-sw/${{ matrix.build.dir }}/mlir_reports/ttir.mlir ${{ steps.strings.outputs.perf_report_path }}/ttir.mlir
          fi
        fi

        echo "Dump ttnn to report"
        if [ "${{ matrix.build.project }}" == "tt-torch" ]; then
          cp ./model_flatbuffer/${{ matrix.build.name }}.ttnn ${{ matrix.build.name }}.ttnn
        fi
        source ttrt-venv/bin/activate
        echo "Run ttrt read"
        ttrt read --ignore-version --section mlir ${{ matrix.build.name }}.ttnn --read-file ttnn_dump.json > /dev/null 2>&1
        echo "Extract MLIR from ttnn"
        python benchmark/extract_mlir.py ttnn_dump.json ttnn.mlir
        cp ttnn.mlir ${{ steps.strings.outputs.perf_report_path }}/ttnn.mlir

    - name: Upload TTIR MLIR separately
      id: upload-ttir-mlir
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: ttir-mlir-${{ steps.fetch-job-id.outputs.job_id }}
        path: ${{ steps.strings.outputs.perf_report_path }}/ttir.mlir
        compression-level: 0

    - name: Upload TTNN MLIR separately
      id: upload-ttnn-mlir
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: ttnn-mlir-${{ steps.fetch-job-id.outputs.job_id }}
        path: ${{ steps.strings.outputs.perf_report_path }}/ttnn.mlir
        compression-level: 0

    - name: Run Device Perf
      shell: bash
      run: |
        source ttrt-venv/bin/activate
        echo "Save artifacts"
        ttrt query --save-artifacts
        chmod ugo+x ./benchmark/ttrt_perf.sh
        echo "Run ttrt perf"
        ./benchmark/ttrt_perf.sh ${{ matrix.build.name }}.ttnn ${{ steps.strings.outputs.perf_report_path }}/benchmark_${{ matrix.build.project }}_e2e_${{ matrix.build.name }}_${{ matrix.build.bs }}_${{ matrix.build.lp }}_${{ steps.fetch-job-id.outputs.job_id }}.json || true

    - name: Add MLIR URLs to perf report
      shell: bash
      run: |
        PERF_REPORT_FILE="${{ steps.strings.outputs.perf_report_path }}/benchmark_${{ matrix.build.project }}_e2e_${{ matrix.build.name }}_${{ matrix.build.bs }}_${{ matrix.build.lp }}_${{ steps.fetch-job-id.outputs.job_id }}.json"
        TTIR_URL=${{ steps.upload-ttir-mlir.outputs.artifact-url }}
        TTNN_URL=${{ steps.upload-ttnn-mlir.outputs.artifact-url }}
        python benchmark/add_mlir_urls.py "$PERF_REPORT_FILE" "$TTIR_URL" "$TTNN_URL"

    - name: Upload Perf Report
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: perf-reports-${{ steps.fetch-job-id.outputs.job_id }}
        path: ${{ steps.strings.outputs.perf_report_path }}
